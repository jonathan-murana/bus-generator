{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bus proyect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: initialization\n",
    "### library importation, constant definition and data loading from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "pd.set_option('precision', 2)\n",
    "from functools import reduce\n",
    "import os\n",
    "from operator import truediv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "constants definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower_limit_time_range=1200\n",
    "upper_limit_time_range=1400\n",
    "line_frequency_bound=10\n",
    "T = 120\n",
    "quantile_freq = 0.1\n",
    "bus_capacity = 5\n",
    "\n",
    "DEBUG = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading data from files - static information from catalogodatos.gub.uy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://catalogodatos.gub.uy/dataset/horarios-omnibus-urbanos-por-parada-stm\n",
    "bus_schedules_by_stops = pd.read_csv('files/uptu_pasada_variante.csv',sep=';')\n",
    "\n",
    "# https://catalogodatos.gub.uy/dataset/horarios-omnibus-urbanos-por-parada-stm\n",
    "uptu_pasada_circular = pd.read_csv('files/uptu_pasada_circular.csv',sep=';')\n",
    "\n",
    "# https://catalogodatos.gub.uy/dataset/transporte-colectivo-paradas-y-puntos-de-control\n",
    "stops_ubication = gpd.read_file('files/v_uptu_paradas.shx')\n",
    "stops_ubication = pd.DataFrame(stops_ubication)\n",
    "\n",
    "# https://catalogodatos.gub.uy/dataset/transporte-colectivo-paradas-y-puntos-de-control\n",
    "recorridos_omnibus = gpd.read_file('files/v_uptu_lsv.dbf')\n",
    "recorridos_omnibus = pd.DataFrame(recorridos_omnibus)\n",
    "\n",
    "# https://catalogodatos.gub.uy/dataset/transporte-colectivo-paradas-y-puntos-de-control\n",
    "ubicacion_puntos_control = gpd.read_file('files/v_uptu_controles.dbf')\n",
    "ubicacion_puntos_control = pd.DataFrame(ubicacion_puntos_control)\n",
    "\n",
    "# https://catalogodatos.gub.uy/dataset/lineas-omnibus-origen-y-destino\n",
    "origen_destino_lineas = gpd.read_file('files/v_uptu_lsv_destinos.dbf')\n",
    "origen_destino_lineas = pd.DataFrame(origen_destino_lineas)\n",
    "\n",
    "# https://catalogodatos.gub.uy/dataset/lineas-omnibus-origen-y-destino\n",
    "recorridos_variantes_no_maximales = gpd.read_file('files/uptu_variante_no_maximal.dbf')\n",
    "recorridos_variantes_no_maximales = pd.DataFrame(recorridos_variantes_no_maximales)\n",
    "\n",
    "if DEBUG:\n",
    "    display(bus_schedules_by_stops.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading data from files - demand for defined period from Renzo (TODO: it should be calculated )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: pedir a Renzo el calculo de esto\n",
    "\n",
    "#file_name='files/trasbordos.csv'\n",
    "#transfers = pd.read_csv(file_name,sep=',')\n",
    "#transfers.columns = ['stop_id_1','stop_id_2','line_id_1','line_id_2','n_transfers']\n",
    "#transfers_all_day = transfers\n",
    "\n",
    "#file_name='files/trasbordos_' + str(lower_limit_time_range) + '_' + str(upper_limit_time_range) +'.csv'\n",
    "#transfers = pd.read_csv(file_name,sep=',')\n",
    "#transfers.columns = ['stop_id_1','stop_id_2','line_id_1','line_id_2','n_transfers']\n",
    "#transfers_1200_1400 = transfers\n",
    "\n",
    "#SOLO ESTE SE USA\n",
    "file_name='files/trasbordos_1200_1400_200.csv'\n",
    "transfers = pd.read_csv(file_name,sep=',')\n",
    "transfers.columns = ['stop_id_1','stop_id_2','line_id_1','line_id_2','n_transfers']\n",
    "trasbordos_1200_1400_200 = transfers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: proccesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the representation of the scenario is the following (output file structure): \n",
    "\n",
    "4 3 120 //lines sync_points period\n",
    "\n",
    "1 2 2 100 3 10 15 16 0 // line1 line2 dist demand wb Wb TTl1 TTl2 bach\n",
    "\n",
    "2 3 1  50 5  8 18 22 0\n",
    "\n",
    "3 4 1  75 5  8 18 22 0\n",
    "\n",
    "5 15\n",
    "\n",
    "4 12\n",
    "\n",
    "4 10\n",
    "\n",
    "4 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lines with low frecuency are filtered (if it are below line_frequency_bound )\n",
    "\n",
    "'ordinal==1' is the first bus stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bus_schedules_by_stops_aux = bus_schedules_by_stops.query('ordinal==1').copy()\n",
    "bus_schedules_by_stops_aux['count'] = 1\n",
    "bus_schedules_by_stops_aux = bus_schedules_by_stops_aux.groupby('cod_variante').sum().reset_index()\n",
    "bus_schedules_by_stops_aux=bus_schedules_by_stops_aux[bus_schedules_by_stops_aux\n",
    "                                                      ['count'] > line_frequency_bound]['cod_variante']\n",
    "bus_schedules_by_stops = bus_schedules_by_stops[bus_schedules_by_stops['cod_variante']\n",
    "                                                .isin(bus_schedules_by_stops_aux)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same filter for transtfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_transfers(transfers):\n",
    "    transfers = transfers[transfers['line_id_1'].isin(bus_schedules_by_stops_aux)] \n",
    "    transfers = transfers[transfers['line_id_2'].isin(bus_schedules_by_stops_aux)]\n",
    "    return transfers\n",
    "\n",
    "#transfers_all_day_with_max = filter_transfers(transfers_all_day_with_max)\n",
    "#transfers_1200_1400 = filter_transfers(transfers_1200_1400)\n",
    "trasbordos_1200_1400_200 = filter_transfers(trasbordos_1200_1400_200)\n",
    "#trasbordos_1200_1400_200\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting some bus stop data for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#time1-time2\n",
    "def min_desde_cero(time):\n",
    "    hours = time // 100\n",
    "    minutes = time % 100\n",
    "    minutos_desde_la_cero_hora=hours*60+minutes\n",
    "    return minutos_desde_la_cero_hora\n",
    "\n",
    "#if DEBUG:\n",
    "    #horas = bus_schedules_by_stops.query(\"cod_variante==7929 and cod_ubic_parada == 582\").copy()\n",
    "    #horas['min_desde_cero'] = horas.apply(lambda e: min_desde_cero(e.hora),axis = 1)\n",
    "    #horas = horas.sort_values(by=['frecuencia'])\n",
    "    #horas['min_desde_cero'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complete transfers with parents lines, through merging with recorridos_variantes_no_maximales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display(recorridos_variantes_no_maximales.head())\n",
    "\n",
    "def complete_maximals(transfers):\n",
    "    transfers_with_no_max = transfers.copy()\n",
    "    transfers_with_no_max =  pd.merge(transfers_with_no_max, recorridos_variantes_no_maximales, \n",
    "                     how='left', left_on=['line_id_1'], right_on=['COD_VARIAN'])\n",
    "    transfers_with_no_max = transfers_with_no_max[['stop_id_1','stop_id_2',\n",
    "                                                   'line_id_1','line_id_2','n_transfers',\n",
    "                                                  'COD_VAR_01',]]\n",
    "    transfers_with_no_max.columns = ['stop_id_1','stop_id_2',\n",
    "                                                   'line_id_1','line_id_2','n_transfers',\n",
    "                                                  'line_id_1_var_01',]\n",
    "\n",
    "    transfers_with_no_max =  pd.merge(transfers_with_no_max, recorridos_variantes_no_maximales, \n",
    "                     how='left', left_on=['line_id_2'], right_on=['COD_VARIAN'])\n",
    "\n",
    "    transfers_with_no_max = transfers_with_no_max[['stop_id_1','stop_id_2',\n",
    "                                                   'line_id_1','line_id_2','n_transfers','line_id_1_var_01',\n",
    "                                                  'COD_VAR_01',]]\n",
    "    transfers_with_no_max.columns = ['stop_id_1','stop_id_2',\n",
    "                                                   'line_id_1','line_id_2','n_transfers',\n",
    "                                                  'line_id_1_var_01','line_id_2_var_01',]\n",
    "\n",
    "    transfers_with_no_max['line_id_1_var_01'] =\\\n",
    "                transfers_with_no_max.apply(lambda e: e.line_id_1 \n",
    "                                            if math.isnan(e.line_id_1_var_01)\n",
    "                                            else e.line_id_1_var_01,axis = 1) \n",
    "\n",
    "    transfers_with_no_max['line_id_2_var_01'] =\\\n",
    "                transfers_with_no_max.apply(lambda e: e.line_id_2 \n",
    "                                            if math.isnan(e.line_id_2_var_01) \n",
    "                                            else e.line_id_2_var_01,axis = 1)\n",
    "    return transfers_with_no_max\n",
    "\n",
    "transfers_1200_1400_with_max_200 = complete_maximals(trasbordos_1200_1400_200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate tt (travel time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time1-time2\n",
    "def diff(time1,time2):\n",
    "    hours1 = time1 // 100\n",
    "    minutes1 = time1 % 100\n",
    "    hours2 = time2 // 100\n",
    "    minutes2 = time2 % 100\n",
    "    minutos_desde_la_cero_hora1=hours1*60+minutes1\n",
    "    minutos_desde_la_cero_hora2=hours2*60+minutes2\n",
    "    return minutos_desde_la_cero_hora2 - minutos_desde_la_cero_hora1\n",
    "\n",
    "def obtener_hora_de_salida(frecuencia,bus):\n",
    "    bus=bus[bus.ordinal == 1]\n",
    "    bus=bus[bus['frecuencia'] ==  frecuencia]\n",
    "    bus=bus[bus['tipo_dia'] ==  1]\n",
    "    bus=bus[bus['dia_anterior'] ==  'N'].copy().reset_index()\n",
    "    return bus['hora']\n",
    "\n",
    "def tt(linea,parada,bus_schedules_by_stops):\n",
    "    \n",
    "    query_this_line = \"cod_variante == \"+ str(linea) + \" and tipo_dia == 1 and dia_anterior == 'N'\"\n",
    "    \n",
    "    \n",
    "    query_this_line_and_stop = \"cod_variante == \"+ str(linea) + \\\n",
    "        \" and cod_ubic_parada == \" + str(parada) + \" and tipo_dia == 1 and dia_anterior == 'N'\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    bus_schedules_by_stops_this_line = bus_schedules_by_stops.query(query_this_line)\n",
    "    bus_schedules_by_stops_this_line_and_stop = bus_schedules_by_stops.query(query_this_line_and_stop)\n",
    "    \n",
    "    \n",
    "    hour_threshold_frecuencies = bus_schedules_by_stops_this_line.query('tipo_dia == 1 and ordinal==1 and hora >=' +\n",
    "                                             str(lower_limit_time_range-400) +\n",
    "                                            'and hora <= ' + str(upper_limit_time_range+400)\n",
    "                                            ).copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    bus_schedules_by_stops_this_line =\\\n",
    "    bus_schedules_by_stops_this_line[bus_schedules_by_stops_this_line\n",
    "                                     ['frecuencia'].isin(hour_threshold_frecuencies['frecuencia'])]\n",
    "    bus_schedules_by_stops_this_line_and_stop =\\\n",
    "    bus_schedules_by_stops_this_line_and_stop[bus_schedules_by_stops_this_line_and_stop\n",
    "                                              ['frecuencia'].isin(hour_threshold_frecuencies['frecuencia'])]\n",
    "    \n",
    "    if (bus_schedules_by_stops_this_line_and_stop.size>0):\n",
    "    \n",
    "        bus_schedules_by_stops_this_line_and_stop['hora_salida'] =\\\n",
    "        bus_schedules_by_stops_this_line_and_stop.apply(lambda e: obtener_hora_de_salida(e.frecuencia,\\\n",
    "        bus_schedules_by_stops_this_line),axis = 1) \n",
    "\n",
    "        bus_schedules_by_stops_this_line_and_stop['TT'] =\\\n",
    "        bus_schedules_by_stops_this_line_and_stop.apply(lambda e: diff(e.hora_salida,e.hora),axis = 1) \n",
    "\n",
    "        #    aux2 =\\\n",
    "        #    aux2.groupby(['tipo_dia','cod_variante','cod_ubic_parada']).mean().reset_index()\n",
    "        #    query='cod_variante=='+ str(linea) +' and cod_ubic_parada == '+str(parada)#+' and tipo_dia == 1 '\n",
    "        #    aaa= aux2.query(query)['TT']\n",
    "        #    return aaa\n",
    "\n",
    "        bus_schedules_by_stops_this_line_and_stop =\\\n",
    "        bus_schedules_by_stops_this_line_and_stop.groupby(['tipo_dia','cod_variante','cod_ubic_parada']).mean().reset_index()\n",
    "        query='cod_variante=='+ str(linea) +' and cod_ubic_parada == '+str(parada)#+' and tipo_dia == 1 '\n",
    "        bus_schedules_by_stops_this_line_and_stop= bus_schedules_by_stops_this_line_and_stop.query(query)['TT']\n",
    "        return bus_schedules_by_stops_this_line_and_stop\n",
    "    \n",
    "    #display(bus_schedules_by_stops_this_line_and_stop.head())\n",
    "\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def add_tt1_tt2(transfers_with_no_max):        \n",
    "    transfers_with_no_max['TT1'] =\\\n",
    "    transfers_with_no_max.apply(lambda e: \n",
    "                                         tt(e.line_id_1,e.stop_id_1,bus_schedules_by_stops),axis = 1).copy() \n",
    "    transfers_with_no_max['TT2'] =\\\n",
    "            transfers_with_no_max.apply(lambda e: tt(e.line_id_2,e.stop_id_2,\n",
    "                                                      bus_schedules_by_stops),axis = 1).copy() \n",
    "    \n",
    "    _aux = transfers_with_no_max.query('TT1 < 0 or TT2 < 0')\n",
    "    _aux = _aux.reset_index()\n",
    "    #display(_aux)\n",
    "    #display(len(_aux.index))\n",
    "        \n",
    "    if _aux.size > 0:\n",
    "        print(\"oops... i could not calculate TTL for \"+ str(len(_aux.index)) + \" lines. It going to be discarted\")\n",
    "    \n",
    "    transfers_with_no_max = transfers_with_no_max.query('TT1 >= 0 and TT2 >= 0')\n",
    "    \n",
    "    \n",
    "    return transfers_with_no_max\n",
    "\n",
    "\n",
    "transfers_1200_1400_max_tts_200 = add_tt1_tt2(transfers_1200_1400_with_max_200).copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge transfers and stops ubication to get stops geometriy (coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sptop_coors(transfers_with_no_max):    \n",
    "    transfers_with_stop_coors = transfers_with_no_max.copy()\n",
    "    transfers_with_stop_coors =  pd.merge(transfers_with_stop_coors, stops_ubication, \n",
    "                     how='left', left_on=['stop_id_1','line_id_1_var_01'], right_on=['COD_UBIC_P',\n",
    "                                                                                     'COD_VARIAN'])\n",
    "    transfers_with_stop_coors = transfers_with_stop_coors[['line_id_1','line_id_2','stop_id_1',\n",
    "                                                           'stop_id_2','line_id_1_var_01',\n",
    "                                                             'line_id_2_var_01','n_transfers','geometry','TT1','TT2']]\n",
    "    transfers_with_stop_coors.columns = ['line_id_1','line_id_2','stop_id_1','stop_id_2',\n",
    "                                         'line_id_1_var_01','line_id_2_var_01','n_transfers',\n",
    "                                          'line_1_geometry','TT1','TT2']\n",
    "    transfers_with_stop_coors = pd.merge(transfers_with_stop_coors, stops_ubication, \n",
    "                     how='left', left_on=['stop_id_2','line_id_2_var_01'], right_on=['COD_UBIC_P',\n",
    "                                                                                     'COD_VARIAN'])\n",
    "    transfers_with_stop_coors = transfers_with_stop_coors[['line_id_1','line_id_2','stop_id_1',\n",
    "                                                           'stop_id_2','line_id_1_var_01',\n",
    "                                                             'line_id_2_var_01','n_transfers',\n",
    "                                                           'line_1_geometry','geometry','TT1','TT2']]\n",
    "    transfers_with_stop_coors = transfers_with_stop_coors[['line_id_1',\n",
    "                                                             'line_id_2','n_transfers',\n",
    "                                                             'line_1_geometry',\n",
    "                                                             'geometry','TT1','TT2']]\n",
    "    transfers_with_stop_coors.columns = ['line_id_1',\n",
    "                                        'line_id_2','n_transfers','line_1_geometry',\n",
    "                                        'line_2_geometry','TT1','TT2']\n",
    "    return transfers_with_stop_coors\n",
    "\n",
    "transfers_1200_1400_max_tts_coors_200 = add_sptop_coors(transfers_1200_1400_max_tts_200)\n",
    "\n",
    "if DEBUG:\n",
    "    display(transfers_1200_1400_max_tts_coors_200.head(1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "plotting stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    #in\n",
    "    transfers_with_stop_coors = transfers_1200_1400_max_tts_coors_200\n",
    "\n",
    "    gdf_bus_stops=gpd.read_file(\"files/v_uptu_paradas/v_uptu_paradas.shp\")\n",
    "    #gdf_bus_stops=gdf_bus_stops.to_crs(epsg=4326) \n",
    "    gdf_streets=gpd.read_file(\"files/v_mdg_vias/v_mdg_vias.shp\")\n",
    "    #gdf_streets=gdf_streets.to_crs(epsg=4326) \n",
    "    bus_schedules_by_stops_aux = bus_schedules_by_stops.copy()\n",
    "    bus_schedules_by_stops_aux = bus_schedules_by_stops_aux.groupby(['cod_variante','cod_ubic_parada'], sort=False).max().reset_index()\n",
    "\n",
    "    #bus_schedules_by_stops_aux = pd.merge(bus_schedules_by_stops_aux, frequency, \n",
    "    #                 how='right', left_on=['cod_variante'], right_on=['linea'])\n",
    "\n",
    "    gdf_bus_stops = pd.merge(gdf_bus_stops, bus_schedules_by_stops_aux, \n",
    "                     how='right', left_on=['COD_UBIC_P'], right_on=['cod_ubic_parada'])\n",
    "\n",
    "    geometry = transfers_with_stop_coors['line_1_geometry']\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    syncs1 = gpd.GeoDataFrame(transfers_with_stop_coors, crs=crs, geometry=geometry)\n",
    "    geometry = transfers_with_stop_coors['line_2_geometry']\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    syncs2 = gpd.GeoDataFrame(transfers_with_stop_coors, crs=crs, geometry=geometry)\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    gdf_streets.plot(ax=ax,linewidth=0.4,color=\"black\",label=\"streets\")\n",
    "    gdf_bus_stops.plot(ax=ax,color=\"blue\",markersize=3,label=\"bus stops\")\n",
    "    syncs = syncs1.append(syncs2)\n",
    "    syncs.plot(ax=ax,color=\"red\",markersize=80,label=\"sync. point\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"longitude\")\n",
    "    plt.ylabel(\"latitude\")\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    #display(gdf_bus_stops.head())\n",
    "    plt.savefig(\"figs/paradas_buses.png\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caculate distance between stops of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dist(geometry_1,geometry_2):\n",
    "    return geometry_1.distance(geometry_2)\n",
    "\n",
    "def add_dist(transfers_with_stop_coors):\n",
    "    transfers_with_stop_coors['dist'] =\\\n",
    "    transfers_with_stop_coors.apply(lambda e:dist(e.line_1_geometry,e.line_2_geometry),axis = 1) \n",
    "    transfers_with_stop_coors.head(20)\n",
    "    return transfers_with_stop_coors\n",
    "    \n",
    "#transfers_all_day_max_tts_coors_dist = add_dist(transfers_all_day_max_tts_coors)\n",
    "#transfers_1200_1400_max_tts_coors_dist = add_dist(transfers_1200_1400_max_tts_coors)\n",
    "transfers_1200_1400_max_tts_coors_dist_200 = add_dist(transfers_1200_1400_max_tts_coors_200)\n",
    "\n",
    "if DEBUG:\n",
    "    #display(transfers_all_day_max_tts_coors_dist.head(1))\n",
    "    #display(transfers_1200_1400_max_tts_coors_dist.head(1))\n",
    "    #display(transfers_1200_1400_max_tts_coors_dist_200.head(1))\n",
    "    ax = transfers_1200_1400_max_tts_coors_dist_200['dist'].plot(\n",
    "        title='distance between stops in meter')\n",
    "    ax.set_xlabel(\"SP ids\")\n",
    "    ax.set_ylabel(\"distance (M)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format points input file\n",
    "\n",
    "intersection points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(transfers_with_stop_coors):\n",
    "    points = transfers_with_stop_coors[['line_id_1','line_id_2','dist','n_transfers','TT1','TT2']]\n",
    "    points.columns = ['line_i','line_j','dist','demand','TT_i','TT_j']\n",
    "    points.insert(4, 'wb',5)\n",
    "    points.insert(5, 'wB', 10)\n",
    "    #points.insert(6, 'TT_i', np.zeros(len(points)))\n",
    "    #points.insert(7, 'TT_j', np.zeros(len(points)))\n",
    "    points.insert(8, 'BP', np.zeros(len(points)))\n",
    "    return points\n",
    "\n",
    "#points_all_day = dist(transfers_all_day_max_tts_coors_dist)\n",
    "#points_1200_1400 = dist(transfers_1200_1400_max_tts_coors_dist)\n",
    "points_1200_1400_200 = dist(transfers_1200_1400_max_tts_coors_dist_200)\n",
    "\n",
    "if DEBUG:\n",
    "    #display(points_all_day.head(1))\n",
    "    #display(points_1200_1400.head())\n",
    "    display(points_1200_1400_200.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frequency\n",
    "\n",
    "justificar modelo de frecuencias constantes calculando promedio y desviacion respecto a frecuencias reales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_min_avg_line_frec( linea ,bus_schedules_by_stops,lower_limit_time_range,upper_limit_time_range):\n",
    "    resultado=bus_schedules_by_stops.query('tipo_dia==1 and ordinal==1 and dia_anterior== \"N\" and cod_variante=='\n",
    "                                           +str(linea)+\n",
    "                                           ' and hora>='+str(lower_limit_time_range-400)+' and hora<='\n",
    "                                           +str(upper_limit_time_range+400))\n",
    "    \n",
    "    \n",
    "    #print(resultado.query('cod_variante == 1276')['hora'])\n",
    "    \n",
    "\n",
    "    resultadoHoraTolistSinRepetidos=list(set(resultado.hora.tolist()))\n",
    "    resultadoHoraTolistSinRepetidos = sorted(resultadoHoraTolistSinRepetidos)\n",
    "    cant_rows = len(resultadoHoraTolistSinRepetidos)\n",
    "    \n",
    "    maxima_fecuencia = 0\n",
    "    minima_frecuencia = 10000000\n",
    "    lista_frecuencias_minutos_linea=[]\n",
    "    \n",
    "    for i in range(cant_rows-1):\n",
    "        time1=resultadoHoraTolistSinRepetidos[i]\n",
    "        time2=resultadoHoraTolistSinRepetidos[i+1]\n",
    "        hours1 = time1 // 100\n",
    "        minutes1 = time1 % 100\n",
    "        hours2 = time2 // 100\n",
    "        minutes2 = time2 % 100\n",
    "        minutos_desde_la_cero_hora1=hours1*60+minutes1\n",
    "        minutos_desde_la_cero_hora2=hours2*60+minutes2\n",
    "        resta = minutos_desde_la_cero_hora2-minutos_desde_la_cero_hora1\n",
    "        lista_frecuencias_minutos_linea.append(resta)\n",
    "        if(maxima_fecuencia<resta):\n",
    "            maxima_fecuencia=resta\n",
    "    #elimina los duplicados    \n",
    "    lista_frecuencias_minutos_linea = list(set(lista_frecuencias_minutos_linea)) \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    retornero =\t{\n",
    "      \"min\": min(lista_frecuencias_minutos_linea),\n",
    "      \"max\": maxima_fecuencia,\n",
    "      \"avg\": sum(lista_frecuencias_minutos_linea)/len(lista_frecuencias_minutos_linea)\n",
    "    }\n",
    "    return retornero\n",
    "\n",
    "def frequencies(points):\n",
    "\n",
    "    lista_min = []\n",
    "    lista_max = []\n",
    "    lista_avg = []\n",
    "    lineas_en_orden_procesado=[]\n",
    "\n",
    "\n",
    "\n",
    "    # list of used stops\n",
    "    aux_lines_i =  pd.DataFrame({'linea': points['line_i'] })\n",
    "    aux_lines_j =  pd.DataFrame({'linea': points['line_j'] })\n",
    "    aux_lines = aux_lines_i.append(aux_lines_j)\n",
    "    aux_lines = aux_lines.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    frequency = aux_lines.copy()\n",
    "\n",
    "    #display(lista_linea)\n",
    "\n",
    "    for i in range(len(frequency)):\n",
    "       lista_linea=frequency.linea.tolist()\n",
    "       min_max_promedio= max_min_avg_line_frec(lista_linea[i],bus_schedules_by_stops,\n",
    "                                               lower_limit_time_range,upper_limit_time_range)\n",
    "       lista_min.append(min_max_promedio[\"min\"])\n",
    "       lista_max.append(min_max_promedio[\"max\"])\n",
    "       lista_avg.append(min_max_promedio[\"avg\"])\n",
    "       lineas_en_orden_procesado.append(lista_linea[i])\n",
    "\n",
    "    lineas_min_max_avg=pd.DataFrame({'linea': lineas_en_orden_procesado, 'h': lista_min, 'H': lista_max, 'avg': lista_avg})\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    frec_h_H_avg =  pd.merge(frequency, lineas_min_max_avg, \n",
    "                     how='left', left_on=['linea'], right_on=['linea'])\n",
    "    \n",
    "    # limpieza\n",
    "    \n",
    "    quantile = frec_h_H_avg['h'].quantile(quantile_freq)    \n",
    "    frec_h_H_avg['h'] = frec_h_H_avg.apply(lambda e: int(math.ceil(quantile)) if e.h < quantile else int(e.h),axis = 1)\n",
    "    \n",
    "    #display(pd.DataFrame(data={'h': lista_frecuencias_minutos_linea}).quantile(0.05))\n",
    "    #display(min(lista_frecuencias_minutos_linea))\n",
    "    \n",
    "    return frec_h_H_avg\n",
    "\n",
    "#frequencies_all_day = frequencies(points_all_day)\n",
    "#frequencies_1200_1400 = frequencies(points_1200_1400)\n",
    "frequencies_1200_1400_200 = frequencies(points_1200_1400_200)\n",
    "\n",
    "if DEBUG:\n",
    "    #display(frequencies_all_day.head(20))\n",
    "    #display(frequencies_1200_1400.head())\n",
    "    #display(frequencies_1200_1400_200.shape[0])\n",
    "    frequencies_1200_1400_200.plot(title=\"h,H and avg\",x=\"linea\", y=[\"h\", \"avg\",\"H\"], kind=\"bar\",figsize=(20,10))\n",
    "    ax.set_xlabel(\"line\")\n",
    "    ax.set_ylabel(\"minutes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate W w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ws(points,frecs):\n",
    "\n",
    "    points_merge_frec_h_H_avg_i= pd.merge(points, frecs, \n",
    "                               how='left', left_on=['line_i'], right_on=['linea'])\n",
    "\n",
    "    points_merge_frec_h_H_avg_j= pd.merge(points, frecs, \n",
    "                               how='left', left_on=['line_j'], right_on=['linea'])\n",
    "\n",
    "    #muestro tabla de frequencys para parejas de lineas\n",
    "    frecueni_j=pd.DataFrame({'line_i': points.line_i.tolist(),\n",
    "                         'line_j': points.line_j.tolist(),\n",
    "                         'h_i': points_merge_frec_h_H_avg_i.h.tolist(),\n",
    "                         'Hi': points_merge_frec_h_H_avg_i.H.tolist(),\n",
    "                         'h_j': points_merge_frec_h_H_avg_j.h.tolist(),\n",
    "                         'Hj': points_merge_frec_h_H_avg_j.H.tolist(),\n",
    "                         'avg_i': points_merge_frec_h_H_avg_i.avg.tolist(),\n",
    "                         'avg_j': points_merge_frec_h_H_avg_j.avg.tolist()\n",
    "\n",
    "                          }) \n",
    "\n",
    "\n",
    "    #display(frecueni_j)\n",
    "\n",
    "    #wb es el minimo de los h de las dos lineas del punto\n",
    "    minimasFrecs=[]\n",
    "    for i in range(len(frecueni_j)):\n",
    "        minimasFrecs.append(min(points_merge_frec_h_H_avg_i.h.tolist()[i],points_merge_frec_h_H_avg_j.h.tolist()[i]))\n",
    "\n",
    "    #display(frec_h_H_avg.H.tolist())\n",
    "    #display(points_merge_frec_h_H_avg)\n",
    "    #wB se setea por el H de la frequency \n",
    "    points=pd.DataFrame({'line_i': points.line_i.tolist(),\n",
    "                         'line_j': points.line_j.tolist(),\n",
    "                         'dist': points.dist.tolist(),\n",
    "                         'demand': points.demand.tolist(),\n",
    "                         'wb': minimasFrecs,\n",
    "                         'wB': points_merge_frec_h_H_avg_j.H.tolist(),\n",
    "                         'TT_i': points.TT_i.tolist(),\n",
    "                         'TT_j': points.TT_j.tolist(),\n",
    "                         'BP': points.BP.tolist(),\n",
    "                          })  \n",
    "    return points\n",
    "    \n",
    "#points_all_day = add_ws(points_all_day,frequencies_all_day)\n",
    "#points_1200_1400 = add_ws(points_1200_1400,frequencies_1200_1400)\n",
    "points_1200_1400_200 = add_ws(points_1200_1400_200,frequencies_1200_1400_200)\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    #display(frequencies_all_day.head(20))\n",
    "    #display(frequencies_1200_1400.head())\n",
    "    #display(frequencies_1200_1400_200.shape[0])\n",
    "    ax = points_1200_1400_200.plot(title=\"wB and Wb\",y=[\"wb\", \"wB\"], kind=\"bar\",figsize=(20,10))\n",
    "    ax.set_xlabel(\"line\")\n",
    "    ax.set_ylabel(\"SP id\")\n",
    "    \n",
    "\n",
    "#display(points_all_day.head(2))\n",
    "#display(points_1200_1400.head(2))\n",
    "#display(points_1200_1400_200.head(2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering lines that arrive out of period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def esta_en_freq(linea,frecs):\n",
    "#    return len(frecs[frecs['linea'] == linea])\n",
    "\n",
    "#def filter_out_of_period(points,frecs):\n",
    "#    frecs = frecs[frecs['avg'] < 60]\n",
    "#    frecs = frecs[frecs['H'] < 120]\n",
    "#    points['i_ok'] = points.apply(lambda e: esta_en_freq(e.line_i,frecs),axis = 1)\n",
    "#    points['j_ok'] = points.apply(lambda e: esta_en_freq(e.line_j,frecs),axis = 1)\n",
    "#    points=points[points['i_ok'] == 1 & points['j_ok']]\n",
    "#    points = points[['line_i','line_j','dist','demand','wb','wB','TT_i','TT_j','BP']]\n",
    "#    lineas_en_puntos =   points.line_i.append(points.line_j)\n",
    "#    frecs = frecs[frecs['linea'].isin(lineas_en_puntos)]\n",
    "#    frecs = frecs.copy()\n",
    "#    points = points.copy()\n",
    "#    return points,frecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "int(ceil) to distance and time travel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int_ceil(points):\n",
    "    points['dist'] = points.apply(lambda e:math.ceil(e.dist),axis = 1)\n",
    "    points['TT_i'] = points.apply(lambda e:math.ceil(e.TT_i),axis = 1) \n",
    "    points['TT_j'] = points.apply(lambda e:math.ceil(e.TT_j),axis = 1) \n",
    "    points['BP'] = points.apply(lambda e:math.ceil(e.BP),axis = 1) \n",
    "    return points\n",
    "    \n",
    "#points_1200_1400 = to_int_ceil(points_1200_1400)\n",
    "#points_all_day = to_int_ceil(points_all_day)\n",
    "points_1200_1400_200 = to_int_ceil(points_1200_1400_200)\n",
    "\n",
    "if DEBUG:\n",
    "    display(points_1200_1400_200.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scenario generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scenario_array = [None] * (15)\n",
    "\n",
    "def get_escenario(points_1200_1400_200,size,random_state, my_lambda, ordinal):\n",
    "    _points_aux1 = points_1200_1400_200.sample(size,random_state=random_state).copy()\n",
    "    points_aux1 = _points_aux1.copy()\n",
    "    points_aux1['wB'] = _points_aux1['wB'] * my_lambda\n",
    "    points_aux1['wB'] = points_aux1.apply(lambda e:math.ceil(e.wB),axis = 1)\n",
    "    points_aux1 = points_aux1.reset_index()\n",
    "    points_aux1 = points_aux1.drop(columns='index')\n",
    "    frequencies_aux1 = frequencies_1200_1400_200.copy()\n",
    "    lineas_en_puntos = points_aux1.line_i.append(points_aux1.line_j)\n",
    "    frequencies_aux1 = frequencies_aux1[frequencies_aux1['linea'].isin(lineas_en_puntos)]\n",
    "    frequencies_aux1 = frequencies_aux1.reset_index()\n",
    "    frequencies_aux1 = frequencies_aux1.drop(columns='index')\n",
    "    scenario_array = {'name':'BS.12-14.'+str(size)+'.' + \n",
    "                      str(len(frequencies_aux1))+'.120.' +  str(int(my_lambda * 100) ) + '.'+ str(ordinal),\n",
    "                     'ps':points_aux1,'fs':frequencies_aux1}\n",
    "    return scenario_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRUPO A\n",
    "\n",
    "chicos 75, 90, 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scenario_array[0] = get_escenario(points_1200_1400_200,30,1,1,0)\n",
    "scenario_array[1] = get_escenario(points_1200_1400_200,30,2,1,1)\n",
    "scenario_array[2] = get_escenario(points_1200_1400_200,30,3,1,2)\n",
    "scenario_array[3] = get_escenario(points_1200_1400_200,30,4,1,3)\n",
    "scenario_array[4] = get_escenario(points_1200_1400_200,30,5,1,4)\n",
    "\n",
    "scenario_array[5] = get_escenario(points_1200_1400_200,30,6,0.9,0)\n",
    "scenario_array[6] = get_escenario(points_1200_1400_200,30,7,0.9,1)\n",
    "scenario_array[7] = get_escenario(points_1200_1400_200,30,8,0.9,2)\n",
    "scenario_array[8] = get_escenario(points_1200_1400_200,30,9,0.9,3)\n",
    "scenario_array[9] = get_escenario(points_1200_1400_200,30,10,0.9,4)\n",
    "\n",
    "scenario_array[10] = get_escenario(points_1200_1400_200,30,11,0.75,0)\n",
    "scenario_array[11] = get_escenario(points_1200_1400_200,30,12,0.75,1)\n",
    "scenario_array[12] = get_escenario(points_1200_1400_200,30,13,0.75,2)\n",
    "scenario_array[13] = get_escenario(points_1200_1400_200,30,14,0.75,3)\n",
    "scenario_array[14] = get_escenario(points_1200_1400_200,30,15,0.75,4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "medianos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scenario_array[15] = get_escenario(points_1200_1400_200,70,1,1,0)\n",
    "#scenario_array[16] = get_escenario(points_1200_1400_200,70,2,1,1)\n",
    "#scenario_array[17] = get_escenario(points_1200_1400_200,70,3,1,2)\n",
    "#scenario_array[18] = get_escenario(points_1200_1400_200,70,4,1,3)\n",
    "#scenario_array[19] = get_escenario(points_1200_1400_200,70,5,1,4)\n",
    "\n",
    "#scenario_array[20] = get_escenario(points_1200_1400_200,70,6,0.9,0)\n",
    "#scenario_array[21] = get_escenario(points_1200_1400_200,70,7,0.9,1)\n",
    "#scenario_array[22] = get_escenario(points_1200_1400_200,70,8,0.9,2)\n",
    "#scenario_array[23] = get_escenario(points_1200_1400_200,70,9,0.9,3)\n",
    "#scenario_array[24] = get_escenario(points_1200_1400_200,70,10,0.9,4)\n",
    "\n",
    "#scenario_array[25] = get_escenario(points_1200_1400_200,70,11,0.75,0)\n",
    "#scenario_array[26] = get_escenario(points_1200_1400_200,70,12,0.75,1)\n",
    "#scenario_array[27] = get_escenario(points_1200_1400_200,70,13,0.75,2)\n",
    "#scenario_array[28] = get_escenario(points_1200_1400_200,70,14,0.75,3)\n",
    "#scenario_array[29] = get_escenario(points_1200_1400_200,70,15,0.75,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario_array[30] = get_escenario(points_1200_1400_200,110,1,1,0)\n",
    "#scenario_array[31] = get_escenario(points_1200_1400_200,110,2,1,1)\n",
    "#scenario_array[32] = get_escenario(points_1200_1400_200,110,3,1,2)\n",
    "#scenario_array[33] = get_escenario(points_1200_1400_200,110,4,1,3)\n",
    "#scenario_array[34] = get_escenario(points_1200_1400_200,110,5,1,4)\n",
    "\n",
    "#scenario_array[35] = get_escenario(points_1200_1400_200,110,6,0.9,0)\n",
    "#scenario_array[36] = get_escenario(points_1200_1400_200,110,7,0.9,1)\n",
    "#scenario_array[37] = get_escenario(points_1200_1400_200,110,8,0.9,2)\n",
    "#scenario_array[38] = get_escenario(points_1200_1400_200,110,9,0.9,3)\n",
    "#scenario_array[39] = get_escenario(points_1200_1400_200,110,10,0.9,4)\n",
    "\n",
    "#scenario_array[40] = get_escenario(points_1200_1400_200,110,11,0.75,0)\n",
    "#scenario_array[41] = get_escenario(points_1200_1400_200,110,12,0.75,1)\n",
    "#scenario_array[42] = get_escenario(points_1200_1400_200,110,13,0.75,2)\n",
    "#scenario_array[43] = get_escenario(points_1200_1400_200,110,14,0.75,3)\n",
    "#scenario_array[44] = get_escenario(points_1200_1400_200,110,15,0.75,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def get_escenario_bactracking(points_1200_1400_200):\n",
    "#    df1 = points_1200_1400_200[0:1]\n",
    "#    df2 = points_1200_1400_200[25:26]\n",
    "#    df3 = points_1200_1400_200[13:14]\n",
    "#    df4 = points_1200_1400_200[24:25]\n",
    "#    df5 = points_1200_1400_200[15:16]\n",
    "#    df6 = points_1200_1400_200[31:32]\n",
    "#    df7 = points_1200_1400_200[23:24]\n",
    "#    frames = [df1, df2, df3,df4,df5,df6,df7]\n",
    "    \n",
    "#    _points_aux1 = pd.concat(frames)\n",
    "    \n",
    "#    size = 8\n",
    "#    my_lambda = 0.75\n",
    "#    ordinal = 0\n",
    "#    points_aux1 = _points_aux1.copy()\n",
    "#    points_aux1['wB'] = _points_aux1['wB'] * my_lambda\n",
    "#    points_aux1['wB'] = points_aux1.apply(lambda e:math.ceil(e.wB),axis = 1)\n",
    "#    points_aux1 = points_aux1.reset_index()\n",
    "#    points_aux1 = points_aux1.drop(columns='index')\n",
    "#    frequencies_aux1 = frequencies_1200_1400_200.copy()\n",
    "#    lineas_en_puntos = points_aux1.line_i.append(points_aux1.line_j)\n",
    "#    frequencies_aux1 = frequencies_aux1[frequencies_aux1['linea'].isin(lineas_en_puntos)]\n",
    "#    frequencies_aux1 = frequencies_aux1.reset_index()\n",
    "#    frequencies_aux1 = frequencies_aux1.drop(columns='index')\n",
    "#    scenario_array = {'name':'BS.12-14.'+str(size)+'.' + \n",
    "#                      str(len(frequencies_aux1))+'.120.' +  str(int(my_lambda * 100) ) + '.'+ str(ordinal),\n",
    "#                     'ps':points_aux1,'fs':frequencies_aux1}\n",
    "#    return scenario_array\n",
    "\n",
    "#scenario_array[45] = get_escenario_bactracking(points_1200_1400_200)\n",
    "\n",
    "#scenario_array[45]['fs']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapping (real line id -> correlative id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(linea,mapeo_linea):\n",
    "    d = mapeo_linea[mapeo_linea['linea']==linea]['id']\n",
    "    return d.values[0]\n",
    "\n",
    "def mapping(points,frec_h_H_avg):\n",
    "\n",
    "    #recorrer frec_h_H_avg y agregar columna, luego agregar nuevas columnas a points\n",
    "    mapeo_linea_id=[]\n",
    "    #se construye mapeo\n",
    "    rango_lineas_procesadas= range(0,len(frec_h_H_avg))\n",
    "    mapeo_lineas = frec_h_H_avg.copy()\n",
    "    mapeo_lineas['id'] = rango_lineas_procesadas\n",
    "    \n",
    "    points_map = points.copy()\n",
    "    points_map['id_i'] = points.apply(lambda e: get_index(e.line_i,mapeo_lineas),axis = 1)\n",
    "    points_map['id_j'] = points.apply(lambda e: get_index(e.line_j,mapeo_lineas),axis = 1)\n",
    "    \n",
    "    #construye nuevo dataframe con las columnas en otro orden\n",
    "    points_con_ids=pd.DataFrame({\n",
    "        'line_i_old': points_map.line_i.tolist(),\n",
    "        'line_j_old': points_map.line_j.tolist(),        \n",
    "        'line_i': points_map.id_i.tolist(),\n",
    "                                 'line_j': points_map.id_j.tolist(),\n",
    "                                 'dist': points_map.dist.tolist(),\n",
    "                                 'demand': points_map.demand.tolist(),\n",
    "                                 'wb': points_map.wb.tolist(),\n",
    "                                 'wB': points_map.wB.tolist(),\n",
    "                                 'TT_i': points_map.TT_i.tolist(),\n",
    "                                 'TT_j': points_map.TT_j.tolist(),\n",
    "                                 'BP': points_map.BP.tolist(),\n",
    "                                  })  \n",
    "\n",
    "    frec_h_H_avg_ids=pd.DataFrame({'linea': mapeo_lineas.id.tolist(),\n",
    "                                   'h': mapeo_lineas.h.tolist(),\n",
    "                                   'H': mapeo_lineas.H.tolist(),\n",
    "                                   'avg': mapeo_lineas.avg.tolist()\n",
    "                                  })  \n",
    "    \n",
    "    points_con_ids = points_con_ids[['line_i','line_j','dist','demand','wb','wB','TT_i','TT_j','BP',]]\n",
    "    \n",
    "    return points_con_ids, frec_h_H_avg_ids\n",
    "\n",
    "#ps_m, fs_m = mapping(scenario_array[0]['ps'],scenario_array[0]['fs'])\n",
    "#len(scenario_array)\n",
    "for i in range(len(scenario_array)):\n",
    "    ps_m, fs_m = mapping(scenario_array[i]['ps'],scenario_array[i]['fs'])\n",
    "    scenario_array[i]['ps_m'] = ps_m\n",
    "    scenario_array[i]['fs_m'] = fs_m\n",
    "#if DEBUG:\n",
    "#    display(scenario_array[9]['fs_m'].head())\n",
    "#    display(scenario_array[9]['ps_m'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scenario validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(scenario):\n",
    "    for e in scenario['ps_m']['line_i'].tolist():\n",
    "        if not (e in scenario['fs_m']['linea'].tolist()):\n",
    "            return False\n",
    "    for e in scenario['ps_m']['line_j']:\n",
    "        if not (e in scenario['fs_m']['linea']):\n",
    "            return False\n",
    "    for e in scenario['fs_m']['linea']:\n",
    "        if e not in scenario['ps_m']['line_i'].tolist() and e not in scenario['ps_m']['line_j'].tolist():\n",
    "            print(scenario['name'] + ' linea ' + str(e))\n",
    "            return False\n",
    "    #and so on ..    \n",
    "    \n",
    "    return True    \n",
    "\n",
    "globalOk = True\n",
    "for i in range(len(scenario_array)):\n",
    "    if not validate(scenario_array[i]):\n",
    "        globalOk = False\n",
    "        print('error!!')\n",
    "print('global ok: ' + str(globalOk) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remarkable solutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "current system solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_sulution_by_im(frequency):\n",
    "\n",
    "    current_solution_aux = bus_schedules_by_stops.copy()\n",
    "    frecuencias_validas_horarios= current_solution_aux.query('tipo_dia == 1 and ordinal==1 and hora >=' +\n",
    "                                                 str(lower_limit_time_range-400) +\n",
    "                                                'and hora <= ' + str(upper_limit_time_range+400)\n",
    "                                                ).copy()\n",
    "    current_solution_aux = current_solution_aux[current_solution_aux['frecuencia'].isin(\n",
    "        frecuencias_validas_horarios['frecuencia'])].copy()\n",
    "    \n",
    "    current_solution_aux = current_solution_aux.query('ordinal == 1 and tipo_dia==1').copy()\n",
    "    current_solution_aux['siguiente'] = current_solution_aux.hora.shift(-1)\n",
    "    current_solution_aux['diff'] = current_solution_aux.apply(lambda e:diff(\n",
    "        e.hora,e.siguiente),axis = 1) .copy()\n",
    "\n",
    "    current_solution_aux = current_solution_aux[current_solution_aux['cod_variante']\n",
    "                                                .isin(frequency['linea'])].copy()\n",
    "    current_solution_aux = current_solution_aux[current_solution_aux['diff'] > 0].copy()\n",
    "    current_solution_aux = current_solution_aux.groupby('cod_variante').mean().reset_index()\n",
    "    frec_h_H_avg_aux =  pd.merge(frequency, current_solution_aux, \n",
    "                     how='left', left_on=['linea'], right_on=['cod_variante']).copy()\n",
    "\n",
    "    frec_h_H_avg_aux = frec_h_H_avg_aux[['diff']]\n",
    "    frec_h_H_avg_aux=frec_h_H_avg_aux.round(0)\n",
    "    current_solution_aux = frec_h_H_avg_aux\n",
    "    current_solution_aux_imm = current_solution_aux\n",
    "    return current_solution_aux_imm\n",
    "\n",
    "def to_int(sol):\n",
    "    sol['diff'] = sol.apply(lambda e: math.ceil(e),axis = 1)\n",
    "    return  sol\n",
    "\n",
    "for i in range(len(scenario_array)):\n",
    "    c_s = current_sulution_by_im(scenario_array[i]['fs'])\n",
    "    scenario_array[i]['c_s'] = to_int(c_s)\n",
    "    \n",
    "if DEBUG:\n",
    "    display(scenario_array[0]['fs_m'].head())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "current system solution to file (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def file_writing_current_solution_by_im(current_sulution, file_name):\n",
    "#    current_sulution.to_csv(file_name, \n",
    "#                                      sep=' ', encoding='utf-8', index=False, header=False)\n",
    "\n",
    "    \n",
    "#def sol_to_int_ceil(sol):\n",
    "#    sol['diff'] = sol.apply(lambda e: math.ceil(e),axis = 1)\n",
    "#    return sol    \n",
    "\n",
    "#display(sol_to_int_ceil(current_sulution_by_im_all_day).head())\n",
    "\n",
    "#file_writing_current_solution_by_im(sol_to_int_ceil(current_sulution_by_im_all_day),'real_timetable_all_day')\n",
    "#file_writing_current_solution_by_im(sol_to_int_ceil(current_sulution_by_im_1200_1400),'real_timetable_1200_1400')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitness current system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display(frec_h_H_avg_ids_to_file)\n",
    "#display(points_con_ids)\n",
    "#display(current_solution_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FITNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## important variables\n",
    "SPEED = 6\n",
    "T = 120 # minumtos\n",
    "\n",
    "def fitness(current_solution_aux,points_con_ids,fitness_version):\n",
    "    \n",
    "    suma = 0\n",
    "    fitness = 0\n",
    "    wait_time_list = []\n",
    "    mean_wait_time_per_point_list = []\n",
    "    rejected_demand_per_point_list = []\n",
    "    syncs = 0\n",
    "    \n",
    "    for k in range(points_con_ids.shape[0]): # For each synchronization point\n",
    "        \n",
    "        line_i = points_con_ids.line_i[k]\n",
    "        line_j = points_con_ids.line_j[k]\n",
    "        \n",
    "        TT_i = points_con_ids.TT_i[k]\n",
    "        TT_j = points_con_ids.TT_j[k]\n",
    "        \n",
    "        t_dist = (points_con_ids.dist[k]/1000/SPEED*60);\n",
    "        t_dist = int(1 + t_dist)\n",
    "                \n",
    "        demand = points_con_ids.demand[k]\n",
    "\n",
    "        w_b = points_con_ids.wb[k]\n",
    "        W_b = points_con_ids.wB[k]\n",
    "        \n",
    "        x= current_solution_aux['diff'][int(line_i)]\n",
    "        y= current_solution_aux['diff'][int(line_j)]\n",
    "        \n",
    "        trips_i = list(range(int(x),T+1,int(x)))\n",
    "        trips_j = list(range(int(y),T+1,int(y)))\n",
    "        \n",
    "        TT_i = math.floor(TT_i) \n",
    "        TT_j = math.floor(TT_j)\n",
    "        \n",
    "        wait_time_per_point_list = []\n",
    "\n",
    "        rejected_demand_per_point = 0\n",
    "        \n",
    "        for line in range(points_con_ids.shape[0]): \n",
    "            sync_trips_per_line_list = []\n",
    "\n",
    "        for i in range(len(trips_i)):\n",
    "            last_sync_time_j = 0\n",
    "          \n",
    "            wait_time_acum = 0\n",
    "            \n",
    "            for j in range(len(trips_j)):                \n",
    "                \n",
    "                wait_time = (trips_j[j]+TT_j)- (trips_i[i] +TT_i) - t_dist\n",
    "                \n",
    "                if fitness_version == 1:\n",
    "                    if wait_time > y:\n",
    "                        wait_time = y\n",
    "                    if wait_time > 0 and wait_time <= W_b:\n",
    "                        \n",
    "                        #wait_time_per_point_list.append(wait_time)\n",
    "                        #wait_time_list.append(wait_time)\n",
    "                        #syncs = syncs + 1\n",
    "                        \n",
    "                        fitness = fitness +  demand * x\n",
    "                        break\n",
    "                        \n",
    "                                                \n",
    "                #if fitness_version == 2:\n",
    "                #    if wait_time >= 0:\n",
    "                #        wait_time_per_point_list.append(wait_time)\n",
    "                #        wait_time_list.append(wait_time)\n",
    "                #        syncs = syncs + 1\n",
    "                #                                \n",
    "                #        fitness = fitness + wait_time * demand * T / (trips_j[j] - last_sync_time_j)\n",
    "                #        last_sync_time_j = trips_j[j];\n",
    "                #        break\n",
    "                \n",
    "                if fitness_version == 3:\n",
    "                    if wait_time > y:\n",
    "                        wait_time = y\n",
    "                    if wait_time > 0 and wait_time <= W_b:\n",
    "                        \n",
    "                        \n",
    "                #        wait_time_per_point_list.append(wait_time)\n",
    "                #        wait_time_list.append(wait_time)\n",
    "                #        syncs = syncs + 1\n",
    "                        \n",
    "                        fitness = fitness +  min(demand * x, bus_capacity * T)\n",
    "                        \n",
    "                #        if (demand * x > bus_capacity * T):\n",
    "                            #print('*')\n",
    "                #            rejected_demand_per_point = rejected_demand_per_point + (demand * x -bus_capacity * T)\n",
    "                        \n",
    "                #        break\n",
    "                    \n",
    "        #rejected_demand_per_point_list.append(rejected_demand_per_point)\n",
    "        \n",
    "        #if len(wait_time_per_point_list) > 0:\n",
    "        #    mean_wait_time_per_point_list.append(reduce(lambda x, y: x + y, wait_time_per_point_list) \n",
    "        #                                         / len(wait_time_per_point_list))\n",
    "        #else:\n",
    "        #    mean_wait_time_per_point_list.append(0)\n",
    "            \n",
    "    return fitness, wait_time_list, mean_wait_time_per_point_list, syncs, rejected_demand_per_point_list\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "#fit_2_1, list_t_2_1, list_p_2_1 = fitness2(\n",
    "#    current_sulution_by_im_1200_1400,points_1200_1400_mapped)\n",
    "#fitness_current_sulution_by_im_all_day= fitness2(current_sulution_by_im_all_day,points_all_day_mapped)\n",
    "#fitness_current_sulution_by_im_1200_1400 = fitness1(current_sulution_by_im_1200_1400,points_1200_1400_mapped)\n",
    "\n",
    "display(\"fitness 1 =\"+str(fitness(scenario_array[0]['c_s'],scenario_array[0]['ps_m'],1)))\n",
    "display(\"fitness 3 =\"+str(fitness(scenario_array[0]['c_s'],scenario_array[0]['ps_m'],3)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random heuristic solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_solution_str(_f):\n",
    "    sol = ''\n",
    "    for line in range(_f.shape[0]): # For each synchronization point\n",
    "        sol = sol + str(np.random.randint(_f['h'][line],_f['H'][line]+1, size=1)[0])\n",
    "        if line < _f.shape[0]-1:\n",
    "            sol = sol + ' '  \n",
    "    return sol \n",
    "\n",
    "for i in range(len(scenario_array)):\n",
    "    s = scenario_array[i]\n",
    "    s['rand_sol'] = random_solution_str(s['fs_m'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hmax_solution_str(_f):\n",
    "    sol = ''\n",
    "    for line in range(_f.shape[0]): # For each synchronization point\n",
    "        sol = sol + str(_f['H'][line])\n",
    "        if line < _f.shape[0]-1:\n",
    "            sol = sol + ' '            \n",
    "    return sol \n",
    "\n",
    "for i in range(len(scenario_array)):\n",
    "    s = scenario_array[i]\n",
    "    s['hmax_sol'] = hmax_solution_str(s['fs_m'])    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hmin_solution_str(_f):\n",
    "    sol = ''\n",
    "    for line in range(_f.shape[0]): # For each synchronization point\n",
    "        sol = sol + str(_f['h'][line])\n",
    "        if line < _f.shape[0]-1:\n",
    "            sol = sol + ' '\n",
    "    return sol \n",
    "\n",
    "for i in range(len(scenario_array)):\n",
    "    s = scenario_array[i]\n",
    "    s['hmin_sol'] = hmin_solution_str(s['fs_m']) \n",
    "    #display(s['hmin_sol'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demand greddy solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greddy_str(scenario,tipoFitness): #tipoFitness=1=>max, tipoFitness=0=>min \n",
    "    sol = ''\n",
    "    #se odenan los puntos por demanda\n",
    "    puntos_ordenados_demanda=scenario['ps_m'].copy().sort_values(by=['demand'],ascending=False)\n",
    "    hs=[]\n",
    "    Hs=[]\n",
    "    #se extrae vector de frecs para mejor indexado\n",
    "    for k in range(scenario['fs_m'].shape[0]): \n",
    "        hs.append(scenario['fs_m'].h[k])\n",
    "        Hs.append(scenario['fs_m'].H[k])\n",
    "\n",
    "    vectorLIneas_frecsUsadas=list(map(lambda x: int(x),np.zeros(scenario['fs_m'].shape[0])))\n",
    "    \n",
    "   # display(vectorLIneas_frecsUsadas)\n",
    "    sumaFitness=0\n",
    "    for p in range(puntos_ordenados_demanda.shape[0]): # For each synchronization point\n",
    "        mini = puntos_ordenados_demanda.iloc[p:p+1]\n",
    "        line_i=mini['line_i'].values\n",
    "        line_j=mini['line_j'].values\n",
    "      #  display(mini)\n",
    "        point_mini=pd.DataFrame({'line_i': [0],\n",
    "                             'line_j': [1],\n",
    "                             'dist': mini.dist.tolist(),\n",
    "                             'demand': mini.demand.tolist(),\n",
    "                             'wb': mini.wb.tolist(),\n",
    "                             'wB': mini.wB.tolist(),\n",
    "                             'TT_i': mini.TT_i.tolist(),\n",
    "                             'TT_j': mini.TT_j.tolist(),\n",
    "                             'BP': mini.BP.tolist(),\n",
    "                              }) \n",
    "        \n",
    "        #se obtienen los limites de frecuencias para cada linea\n",
    "        hi=hs[line_i[0]]\n",
    "        Hi=Hs[line_i[0]]\n",
    "        hj=hs[line_j[0]]\n",
    "        Hj=Hs[line_j[0]]\n",
    "\n",
    "\n",
    "        list_hi_usar=[]\n",
    "        list_hj_usar=[]\n",
    "        \n",
    "        #se construye la lista de frec a usar para cada linea\n",
    "        \n",
    "        #si todavia no se encontro una frecuencia para la linea i, se pruebab con todas\n",
    "        if( vectorLIneas_frecsUsadas[line_i[0]]==0):\n",
    "            for frei in range(hi, Hi):\n",
    "                list_hi_usar.append(frei)\n",
    "        else:\n",
    "            list_hi_usar.append(vectorLIneas_frecsUsadas[line_i[0]])\n",
    "        \n",
    "        #si todavia no se encontro una frecuencia para la linea j, se pruebab con todas   \n",
    "        if( vectorLIneas_frecsUsadas[line_j[0]]==0):\n",
    "            for frej in range(hj, Hj):\n",
    "                list_hj_usar.append(frej)\n",
    "        else:\n",
    "            list_hj_usar.append(vectorLIneas_frecsUsadas[line_j[0]])\n",
    "            \n",
    "        if(tipoFitness==1):    \n",
    "            mejorFitness=0\n",
    "        if(tipoFitness==2):    \n",
    "            mejorFitness=sys.maxsize\n",
    "        if(tipoFitness==3):    \n",
    "            mejorFitness=0\n",
    "        \n",
    "        frec_i_mejor=0\n",
    "        frec_j_mejor=0\n",
    "        #se itrea con las listas de frecuencias para cada linea\n",
    "        #si ya se había encontrado una frecuencia para alguna linea\n",
    "        #la lista tendra solo un valor (la frecuencia encontrada antes)\n",
    "        for frei in list_hi_usar:\n",
    "            for frej in list_hj_usar:  \n",
    "               # display(frei)\n",
    "               # display(frej)\n",
    "                fit, waits_list, _waits_list_per_points, syncs_trips, _reject = fitness( \n",
    "                    pd.DataFrame(data={'diff': [frei,frej]}),point_mini,tipoFitness) #tipo de modelo\n",
    "                if(fit>mejorFitness and tipoFitness==1):\n",
    "                    frec_i_mejor=frei\n",
    "                    frec_j_mejor=frej\n",
    "                    mejorFitness=fit\n",
    "\n",
    "                if(fit<mejorFitness and tipoFitness==2):\n",
    "                    frec_i_mejor=frei\n",
    "                    frec_j_mejor=frej\n",
    "                    mejorFitness=fit\n",
    "                if(fit>mejorFitness and tipoFitness==3):\n",
    "                    frec_i_mejor=frei\n",
    "                    frec_j_mejor=frej\n",
    "                    mejorFitness=fit\n",
    "        \n",
    "        \n",
    "        sumaFitness=sumaFitness+mejorFitness           \n",
    "        vectorLIneas_frecsUsadas[line_i[0]]=frec_i_mejor\n",
    "        vectorLIneas_frecsUsadas[line_j[0]]=frec_j_mejor\n",
    "        \n",
    "        \n",
    "    #se construye solucion concatenando la lista de frecuencias    \n",
    "    vectorLIneas_frecsUsadas=list(map(lambda x: str(int(x)),vectorLIneas_frecsUsadas))\n",
    "    sol=' '.join(vectorLIneas_frecsUsadas)\n",
    "    \n",
    "    if(False):\n",
    "        display(scenario['name']+\": modelo \"+str(tipoFitness))\n",
    "        display(\"    Fitness: \"+str(sumaFitness))\n",
    "        display(\"    Sol: \"+sol)\n",
    "\n",
    "    return sol \n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(scenario_array)):\n",
    "    s = scenario_array[i]\n",
    "    s['greddy_sol_model1'] = greddy_str(s,1)\n",
    "    s['greddy_sol_model2'] = greddy_str(s,2)\n",
    "    s['greddy_sol_model3'] = greddy_str(s,3)\n",
    "    #display(s['greddy_sol_model1'])\n",
    "\n",
    "#len(scenario_array)   \n",
    "    \n",
    "    \n",
    "#scenario_array[9]['greddy_sol_model3']\n",
    "#display(scenario_array[9]['ps_m'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bracktracking solution TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario_array[45]['ps_m']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_sol(sol_str):\n",
    "    return pd.DataFrame(data={'diff': list(map(lambda x: int(x), sol_str.split(' ')))})\n",
    "\n",
    "def file_writing(s):\n",
    "    #display(s)\n",
    "    folder = 'instances/'\n",
    "    points = s['ps_m']\n",
    "    frequency = s['fs_m']\n",
    "    current_solution = s['c_s']\n",
    "    file_name = s['name']\n",
    "   \n",
    "    d = {\n",
    "     'cantidad_de_lineas': [frequency.shape[0]],\n",
    "     'puntos_sincro': [points.shape[0]],\n",
    "     'periodo': [T],      \n",
    "    }\n",
    "    head = pd.DataFrame(data=d)\n",
    "    \n",
    "    head.to_csv(folder + file_name + '_h', sep=' ', encoding='utf-8', index=False, header=False)\n",
    "    frequency[['h','H']].to_csv(folder + file_name + '_f', sep=' ', encoding='utf-8', index=False, header=False)\n",
    "    points.to_csv(folder + file_name + '_p', sep=' ', encoding='utf-8', index=False, header=False)\n",
    "    \n",
    "\n",
    "    filenames = [folder + file_name + '_h',folder + file_name + '_p',folder + file_name + '_f']\n",
    "    with open(folder + file_name, 'w') as outfile:\n",
    "        for fname in filenames:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "                    \n",
    "    format_to_sol(s['greddy_sol_model3'])['diff'].to_csv(folder +'sol_greedy_model3_'+file_name ,\n",
    "                                          sep=' ', encoding='utf-8', index=False, header=False)\n",
    "    current_solution.to_csv(folder +'sol_real_'+file_name , sep=' ', encoding='utf-8', index=False, header=False)\n",
    "    #import os\n",
    "    os.remove(folder + file_name + '_h')\n",
    "    os.remove(folder + file_name + '_p')\n",
    "    os.remove(folder + file_name + '_f')\n",
    "    \n",
    "    \n",
    "    \n",
    "for i in range(len(scenario_array)):\n",
    "    s = scenario_array[i]\n",
    "    file_writing(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metrics_fitness(solutions,solutions_method, points, fitness_version):\n",
    "    \n",
    "    solution_method = []\n",
    "    cant_trips  = []\n",
    "    array_syncs_trips = []\n",
    "    array_fitness = []\n",
    "    mean_waits_per_trip = []\n",
    "    average_frequency_list = []\n",
    "    \n",
    "    for i in range(len(solutions)):\n",
    "        fit, waits_list, _waits_list_per_points, syncs_trips, _reject = fitness(solutions[i],points,\n",
    "                                                                                fitness_version)    \n",
    "        solution_method.append(solutions_method[i])\n",
    "        \n",
    "        #if len(waits_list) > 0:\n",
    "        #    mean_waits_per_trip.append(reduce(lambda x, y: x + y, waits_list) / len(waits_list))\n",
    "        \n",
    "        cant_trips.append(sum(T/solutions[i]['diff']))\n",
    "        \n",
    "        array_fitness.append(fit)\n",
    "        #array_syncs_trips.append(syncs_trips)\n",
    "        \n",
    "        #average_frequency_list.append(reduce(lambda x, y: x + y, solutions[i]['diff']) \n",
    "        #                                         / solutions[i].shape[0])\n",
    "        \n",
    "\n",
    "    #l = [(x*1.0)/y for x, y in zip(array_syncs_trips, cant_trips)]\n",
    "        \n",
    "\n",
    "    d1 = {'meth': solution_method,\n",
    "         #'mean_wait_per_trip': mean_waits_per_trip,\n",
    "         '# trips': cant_trips,\n",
    "         'fitness': array_fitness,\n",
    "         #'#syncs trips': array_syncs_trips,\n",
    "         #'#syncs / trips': l ,\n",
    "         #'mean freq': average_frequency_list \n",
    "        }\n",
    "    metrics1 = pd.DataFrame(data=d1)\n",
    "    return metrics1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histogram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histogram(solutions, solutions_method, points,fitness_version):\n",
    "    \n",
    "    h = len(solutions) * 6\n",
    "    w = len(solutions) * 4\n",
    "    \n",
    "    fig, axs = plt.subplots(len(solutions), 2, sharey=False, tight_layout=True,figsize=(h,w))\n",
    "    fig.suptitle('Waiting time histogram and cumulative', fontsize=16,y=1.05)\n",
    "    #display(solutions)\n",
    "    for i in range(len(solutions)):\n",
    "        wait_time_all1, wait_time_list1, mean_wait_time_per_point_list1, s, _reject  = fitness(solutions[i],points,fitness_version) \n",
    "        axs[i,0].set_title(solutions_method[i], fontsize=15)\n",
    "        axs[i,0].set_ylabel('trips')\n",
    "        axs[i,0].set_xlabel('waiting time (minutes)')\n",
    "        #axs[i,0].set_xticks(range(max(wait_time_list1)))\n",
    "        axs[i,0].hist(wait_time_list1,bins=max(wait_time_list1))\n",
    "        \n",
    "        axs[i,1].set_title(solutions_method[i], fontsize=15)\n",
    "        axs[i,1].set_ylabel('percentage')\n",
    "        axs[i,1].set_xlabel('waiting time (minutes)')\n",
    "        #axs[i,1].set_xticks(range(max(wait_time_list1)))\n",
    "        axs[i,1].hist(wait_time_list1, bins=max(wait_time_list1), cumulative=True, normed=True)\n",
    "\n",
    "             \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bar function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bars(solutions,solutions_method, points):\n",
    "    fd1  = pd.DataFrame()\n",
    "    fd2  = pd.DataFrame()\n",
    "        \n",
    "    \n",
    "    for i in range(len(solutions)):\n",
    "        fit_1_1, list_t_1_1, list_p_1_1,ss, _reject = fitness(solutions[i],points,3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(list_p_1_1)\n",
    "        \n",
    "        l = len(list_p_1_1)\n",
    "        div = math.ceil(l/2)\n",
    "        \n",
    "        fd1[solutions_method[i]] = list_p_1_1[0:div]\n",
    "        fd2[solutions_method[i]] = list_p_1_1[div:]\n",
    "        \n",
    "        fd1['ind'] = range(0,div)\n",
    "        fd2['ind'] = range(div,l)\n",
    "        fd1.set_index(\"ind\",drop=True,inplace=True)\n",
    "        fd2.set_index(\"ind\",drop=True,inplace=True)\n",
    "        \n",
    "        \n",
    "       \n",
    "    fd1.plot.bar(figsize = (15, 10), rot=0)\n",
    "    fd2.plot.bar(figsize = (15, 10), rot=0)\n",
    "    \n",
    "    #display(fd1)\n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ancilary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def highlight_min(s):\n",
    "    is_min = s == s.min()\n",
    "    #x = list(map(lambda x: is_min, lista.split(' ')))\n",
    "    \n",
    "    return ['background-color: yellow' if v else ' ' for v in is_min]\n",
    "\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else ' ' for v in is_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## titulo\n",
    "display results (metrics)\n",
    "\n",
    "separar metricas por sp sync y no sync\n",
    "\n",
    "formular problema opuesto (minimizar no sincronizados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIO = scenario_array[0]\n",
    "\n",
    "# ae = '7 10 16 5 5 15 8 12 5 12 7 19 19 13 15 8 5 19 16 6 8 8 24 24 24 7 8 11 15 12 5 8 10 5 10 15 20 5 13 12 27 29 59 5 6 20 5 17 20 10 17 15 17 20 8 6 8 7 20 10 17 8 10 17 9 7 24 24 12 40 15 29'\n",
    "\n",
    "solutions = [\n",
    "    SCENARIO['c_s'],\n",
    "    format_to_sol(SCENARIO['greddy_sol_model1']),\n",
    "    #format_to_sol(ae),\n",
    "    format_to_sol(SCENARIO['hmin_sol']),\n",
    "    format_to_sol(SCENARIO['hmax_sol']),\n",
    "    format_to_sol(SCENARIO['rand_sol']),\n",
    "]\n",
    "\n",
    "solutions_method = ['IM', 'greddy', 'hmin_sol', 'hmax_sol', 'rand_sol']\n",
    "metrics = metrics_fitness(solutions, solutions_method, SCENARIO['ps_m'],3)\n",
    "\n",
    "#metrics['fit_norm_by_im'] = metrics.apply(lambda e: 100 * \n",
    "#                                          (e.fitness - metrics.fitness[0]) /metrics.fitness[0],axis = 1)\n",
    "\n",
    "#metrics = metrics.style.apply(highlight_max, subset=['fitness','#syncs / trips'])\n",
    "\n",
    "#metrics.format({'mean_wait_per_trip': '{:4.1f}','# trips': '{:3.0f}',\n",
    "#             'fitness': '{:4.0f}' ,'mean freq': '{:4.1f}','#syncs / trips': '{:4.2f}' ,'fit_norm_by_im': '{:4.2f}' })\n",
    "display(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCENARIO = scenario_array[10]\n",
    "\n",
    "#ae_ax = '7 10 8 5 6 12 8 8 5 6 7 19 19 13 8 8 5 31 10 16 6 5 8 24 16 8 23 6 11 10 8 5 5 12 5 10 10 5 10 5 9 12 27 29 59 5 6 15 5 17 10 20 8 17 8 6 12 7 8 6 8 6 5 10 12 7 10 8 10 7 24 20 12 40 6 10 10 24 20'\n",
    "#ae_ux_pm01 = '7 10 8 5 6 12 8 10 5 6 7 19 16 13 8 8 5 31 10 16 6 5 9 24 16 8 7 6 11 10 8 6 5 12 5 10 10 5 10 5 8 12 24 29 59 5 6 15 5 17 10 20 8 17 8 6 12 7 8 6 8 6 6 10 12 7 10 8 10 7 24 20 12 40 6 10 10 24 20'\n",
    "#ae_ux_pm0025 = '7 10 8 5 6 15 8 12 5 6 5 24 19 13 8 8 7 31 10 16 10 6 8 24 16 8 30 6 11 10 8 5 5 12 5 10 17 5 15 5 9 12 27 29 59 5 6 15 7 17 25 50 8 17 8 6 20 7 8 8 8 6 12 10 15 10 11 8 10 7 24 24 12 60 6 10 17 24 28'\n",
    "\n",
    "#lista = '8 10 16 24 10 24 16 24 20 32 16 33 24 24 24 8 8 31 27 34 10 10 9 48 43 24 30 8 27 17 17 12 8 12 8 10 17 24 20 8 25 16 36 40 56 20 17 24 24 17 45 25 17 17 8 15 12 20 8 6 15 6 38 10 12 7 11 10 9 8 23 20 15 57 12 20 39 36 20'\n",
    "#ae_ux_psq8 = pd.DataFrame(data={'diff': list(map(lambda x: int(x), lista.split(' ')))})\n",
    "\n",
    "#ae_ux_pm01\n",
    "#solutions = [\n",
    "#    SCENARIO['c_s'],\n",
    "#    format_to_sol(SCENARIO['greddy_sol_model3']),\n",
    "#    format_to_sol(ae_ax),\n",
    "#    format_to_sol(ae_ux_pm01),\n",
    "#    format_to_sol(ae_ux_pm0025),\n",
    "#    ae_ux_psq8,\n",
    "#    format_to_sol(SCENARIO['hmin_sol']),\n",
    "#    format_to_sol(SCENARIO['hmax_sol']),\n",
    "#    format_to_sol(SCENARIO['rand_sol']),   \n",
    "#]\n",
    "\n",
    "#solutions_method = ['IM' , \n",
    "#                    'greddy',\n",
    "#                    'ae_ax',\n",
    "#                    'ae_ux_pm01',\n",
    "#                    'ae_ux_pm0025',\n",
    "#                    'ae_ux_psq8',\n",
    "#                    'hmin_sol',\n",
    "#                    'hmax_sol',\n",
    "#                    'rand_sol',\n",
    "#                   ]\n",
    "\n",
    "#metrics = metrics_fitness(solutions, solutions_method, SCENARIO['ps_m'],3)\n",
    "#metrics['fit_norm_by_im'] = metrics.apply(lambda e: 100 * (e.fitness - metrics.fitness[0]) /metrics.fitness[0],axis = 1)\n",
    "#metrics = metrics.style.apply(highlight_max, subset=['fitness','#syncs / trips'])\n",
    "#metrics.format({'mean_wait_per_trip': '{:4.1f}','# trips': '{:3.0f}',\n",
    "#             'fitness': '{:4.0f}' ,'mean freq': '{:4.1f}',\n",
    "#                '#syncs / trips': '{:4.3f}','fit_norm_by_im': '{:4.1f}'})\n",
    "#display(metrics)\n",
    "#display(SCENARIO['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCENARIO = scenario_array[11]\n",
    "\n",
    "#ae = '7 10 8 9 5 12 8 12 5 6 5 19 16 13 8 8 7 31 10 16 6 5 8 24 16 8 7 6 11 10 8 5 5 10 5 10 10 5 10 5 9 12 24 29 59 5 6 15 5 17 10 20 8 17 8 6 12 7 8 6 10 7 5 10 13 7 10 8 10 7 24 20 13 40 6 10 10 24 20'\n",
    "\n",
    "\n",
    "#solutions = [\n",
    "#    SCENARIO['c_s'],\n",
    "#    format_to_sol(SCENARIO['greddy_sol_model3']),\n",
    "#    format_to_sol(ae),\n",
    "#    format_to_sol(SCENARIO['hmin_sol']),\n",
    "#    format_to_sol(SCENARIO['hmax_sol']),\n",
    "#    format_to_sol(SCENARIO['rand_sol']),\n",
    "#]\n",
    "\n",
    "#solutions_method = ['IM' , \n",
    "#                    'greddy',\n",
    "#                    'ae',\n",
    "#                    'hmin_sol',\n",
    "#                    'hmax_sol',\n",
    "#                    'rand_sol',\n",
    "#                   ]\n",
    "\n",
    "#metrics = metrics_fitness(solutions, solutions_method, SCENARIO['ps_m'],3)\n",
    "#metrics['fit_norm_by_im'] = metrics.apply(lambda e: 100 * \n",
    "#                                          (e.fitness - metrics.fitness[0]) /metrics.fitness[0],axis = 1)\n",
    "#metrics = metrics.style.apply(highlight_max, subset=['fitness','#syncs / trips'])\n",
    "#metrics.format({'mean_wait_per_trip': '{:4.1f}','# trips': '{:3.0f}',\n",
    "#             'fitness': '{:4.0f}' ,'mean freq': '{:4.1f}',\n",
    "#                '#syncs / trips': '{:4.3f}','fit_norm_by_im': '{:4.1f}'})\n",
    "\n",
    "#display(metrics)\n",
    "#display(SCENARIO['name'])\n",
    "#display(scenario_array[11]['ps_m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SCENARIO = scenario_array[6]\n",
    "\n",
    "#ae = '7 9 8 5 6 12 7 10 5 8 7 19 16 13 12 8 5 31 11 16 6 5 4 6 4 12 7 4 6 8 8 6 8'\n",
    "\n",
    "#solutions = [\n",
    "#    SCENARIO['c_s'],\n",
    "#    format_to_sol(SCENARIO['greddy_sol_model3']),\n",
    "#    format_to_sol(ae),\n",
    "#    format_to_sol(SCENARIO['hmin_sol']),\n",
    "#    format_to_sol(SCENARIO['hmax_sol']),\n",
    "#    format_to_sol(SCENARIO['rand_sol']),\n",
    "#]\n",
    "\n",
    "#solutions_method = ['IM',\n",
    "#                    'greddy',\n",
    "#                    'ae',\n",
    "#                    'hmin_sol',\n",
    "#                    'hmax_sol',\n",
    "#                    'rand_sol',\n",
    "#                   ]\n",
    "\n",
    "#metrics = metrics_fitness(solutions, solutions_method, SCENARIO['ps_m'],3)\n",
    "#metrics['fit_norm_by_im'] = metrics.apply(lambda e: 100 * \n",
    "#                                          (e.fitness - metrics.fitness[0]) /metrics.fitness[0],axis = 1)\n",
    "#metrics = metrics.style.apply(highlight_max, subset=['fitness','#syncs / trips'])\n",
    "#metrics.format({'mean_wait_per_trip': '{:4.1f}','# trips': '{:3.0f}',\n",
    "#             'fitness': '{:4.0f}' ,'mean freq': '{:4.1f}',\n",
    "#                '#syncs / trips': '{:4.3f}','fit_norm_by_im': '{:4.1f}'})\n",
    "\n",
    "\n",
    "#display(metrics)\n",
    "#display(SCENARIO['name'])\n",
    "#SCENARIO['ps_m']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SCENARIO = scenario_array[7]\n",
    "\n",
    "#ae = '7 9 8 5 6 12 7 10 5 8 7 19 16 13 12 8 5 31 11 16 6 5 4 6 4 12 7 4 6 8 8 6 8'\n",
    "\n",
    "#solutions = [\n",
    "#    SCENARIO['c_s'],\n",
    "#    format_to_sol(SCENARIO['greddy_sol_model3']),\n",
    "#    format_to_sol(ae),\n",
    "#    format_to_sol(SCENARIO['hmin_sol']),\n",
    "#    format_to_sol(SCENARIO['hmax_sol']),\n",
    "#    format_to_sol(SCENARIO['rand_sol']),\n",
    "#]\n",
    "\n",
    "#solutions_method = ['IM',\n",
    "#                    'greddy',\n",
    "#                    'ae',\n",
    "#                    'hmin_sol',\n",
    "#                    'hmax_sol',\n",
    "#                    'rand_sol',\n",
    "#                   ]\n",
    "\n",
    "#metrics = metrics_fitness(solutions, solutions_method, SCENARIO['ps_m'],3)\n",
    "#metrics['fit_norm_by_im'] = metrics.apply(lambda e: 100 * \n",
    "#                                          (e.fitness - metrics.fitness[0]) /metrics.fitness[0],axis = 1)\n",
    "#/'//'metrics = metrics.style.apply(highlight_max, subset=['fitness','#syncs / trips'])\n",
    "#metrics.format({'mean_wait_per_trip': '{:4.1f}','# trips': '{:3.0f}',\n",
    "#             'fitness': '{:4.0f}' ,'mean freq': '{:4.1f}',\n",
    "#                '#syncs / trips': '{:4.3f}','fit_norm_by_im': '{:4.1f}'})\n",
    "\n",
    "\n",
    "#display(metrics)\n",
    "#display(SCENARIO['name'])\n",
    "#SCENARIO['ps_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SCENARIO = scenario_array[8]\n",
    "\n",
    "#ae = '8 9 15 24 10 24 16 25 20 32 13 32 24 24 24 8 9 31 32 37 8 12 7 6 4 20 7 5 6 12 8 6 13'\n",
    "\n",
    "\n",
    "#solutions = [\n",
    "#    SCENARIO['c_s'],\n",
    "#    format_to_sol(SCENARIO['greddy_sol_model3']),\n",
    "#    format_to_sol(ae),\n",
    "#    format_to_sol(SCENARIO['hmin_sol']),\n",
    "#    format_to_sol(SCENARIO['hmax_sol']),\n",
    "#    format_to_sol(SCENARIO['rand_sol']),\n",
    "#]\n",
    "\n",
    "#solutions_method = ['IM',\n",
    "#                    'greddy',\n",
    "#                    'ae',\n",
    "#                    'hmin_sol',\n",
    "#                    'hmax_sol',\n",
    "#                    'rand_sol',\n",
    "#                   ]\n",
    "\n",
    "#metrics = metrics_fitness(solutions, solutions_method, SCENARIO['ps_m'],3)\n",
    "#metrics['fit_norm_by_im'] = metrics.apply(lambda e: 100 * \n",
    "#                                          (e.fitness - metrics.fitness[0]) /metrics.fitness[0],axis = 1)\n",
    "#metrics = metrics.style.apply(highlight_max, subset=['fitness','#syncs / trips'])\n",
    "#metrics.format({'mean_wait_per_trip': '{:4.1f}','# trips': '{:3.0f}',\n",
    "#             'fitness': '{:4.0f}' ,'mean freq': '{:4.1f}',\n",
    "#                '#syncs / trips': '{:4.3f}','fit_norm_by_im': '{:4.1f}'})\n",
    "\n",
    "\n",
    "#display(metrics)\n",
    "#display(SCENARIO['name'])\n",
    "#SCENARIO['ps_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
